- PaGraph: Scaling GNN training on large graphs via computation-aware caching
  - 本文章主要针对大规模GNN训练。当前方案用mini-batch和sampling，在大的graph上进行训练。但是在从CPU载入rich vertices到GPU的时候，他们会有scalability的问题，此时bandwidth极大地
  限制了训练。这篇文章提出了一个系统，在单机多卡上面训练GNN。他利用可用GPU资源作为缓存载体，减少data loading time。他们还设计了一个缓存策略，考虑图结构信息和数据获取模式。最后，
  为了在多个GPU上面分布式计算，他们开发了一个fast GNN-computation-aware partition algorithm来减少交叉切割。
  
- Baechi: fast device placement of machine learning graphs
  - 这个文章应该是follow google那个DRL文章，做device placement。之前的方案会导致训练model-parallelism非常耗时，因为规划算子在不同device这个过程很花时间。这个文章提出了一个系统，
  可以再资源受限的小集群上面运行。系统中包含了一个高效算法，可以极大地加速方案搜索。
